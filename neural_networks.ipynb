{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Netwoks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cultivars</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic_acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity_of_ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total_phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid_phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color_intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315_of_diluted_wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cultivars  Alcohol  Malic_acid   Ash  Alcalinity_of_ash  Magnesium  \\\n",
       "0          1    14.23        1.71  2.43               15.6        127   \n",
       "1          1    13.20        1.78  2.14               11.2        100   \n",
       "2          1    13.16        2.36  2.67               18.6        101   \n",
       "3          1    14.37        1.95  2.50               16.8        113   \n",
       "4          1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total_phenols  Flavanoids  Nonflavanoid_phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color_intensity   Hue  OD280/OD315_of_diluted_wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name = 'Cultivars'\n",
    "\n",
    "feature_cols = [\n",
    "        'Alcohol',\n",
    "        'Malic_acid',\n",
    "        'Ash',\n",
    "        'Alcalinity_of_ash',\n",
    "        'Magnesium',\n",
    "        'Total_phenols',\n",
    "        'Flavanoids',\n",
    "        'Nonflavanoid_phenols',\n",
    "        'Proanthocyanins',\n",
    "        'Color_intensity',\n",
    "        'Hue',\n",
    "        'OD280/OD315_of_diluted_wines',\n",
    "        'Proline'\n",
    "    ]\n",
    "\n",
    "cols = feature_cols.copy()\n",
    "cols.insert(0, class_name)\n",
    "\n",
    "df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', \n",
    "                 header=None, names=cols)\n",
    "\n",
    "features = df.drop(class_name, axis=1)\n",
    "target = df[class_name]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/stepan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/stepan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/stepan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/stepan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/stepan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/stepan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/stepan/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/stepan/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/stepan/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/stepan/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/stepan/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/stepan/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "batch_size = 60\n",
    "num_classes = 3\n",
    "epochs = 150\n",
    "input_dim = len(feature_cols) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(scaled_features, target, test_size=0.3)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical([x-1 for x in y_train.iloc[:].values], num_classes)\n",
    "y_test = keras.utils.to_categorical([x-1 for x in y_test.iloc[:].values], num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 112       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 211\n",
      "Trainable params: 211\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, activation='relu', input_shape=(input_dim,)))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/stepan/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 124 samples, validate on 54 samples\n",
      "Epoch 1/150\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 1.1218 - accuracy: 0.3306 - val_loss: 1.1221 - val_accuracy: 0.3519\n",
      "Epoch 2/150\n",
      "124/124 [==============================] - 0s 54us/step - loss: 1.1041 - accuracy: 0.3629 - val_loss: 1.1076 - val_accuracy: 0.3519\n",
      "Epoch 3/150\n",
      "124/124 [==============================] - 0s 70us/step - loss: 1.0882 - accuracy: 0.4032 - val_loss: 1.0936 - val_accuracy: 0.3519\n",
      "Epoch 4/150\n",
      "124/124 [==============================] - 0s 96us/step - loss: 1.0733 - accuracy: 0.4355 - val_loss: 1.0799 - val_accuracy: 0.3519\n",
      "Epoch 5/150\n",
      "124/124 [==============================] - 0s 92us/step - loss: 1.0581 - accuracy: 0.4677 - val_loss: 1.0675 - val_accuracy: 0.3704\n",
      "Epoch 6/150\n",
      "124/124 [==============================] - 0s 97us/step - loss: 1.0437 - accuracy: 0.4919 - val_loss: 1.0563 - val_accuracy: 0.3704\n",
      "Epoch 7/150\n",
      "124/124 [==============================] - 0s 74us/step - loss: 1.0307 - accuracy: 0.5161 - val_loss: 1.0450 - val_accuracy: 0.3704\n",
      "Epoch 8/150\n",
      "124/124 [==============================] - 0s 105us/step - loss: 1.0177 - accuracy: 0.5242 - val_loss: 1.0343 - val_accuracy: 0.3889\n",
      "Epoch 9/150\n",
      "124/124 [==============================] - 0s 116us/step - loss: 1.0052 - accuracy: 0.5403 - val_loss: 1.0238 - val_accuracy: 0.4074\n",
      "Epoch 10/150\n",
      "124/124 [==============================] - 0s 104us/step - loss: 0.9939 - accuracy: 0.5484 - val_loss: 1.0139 - val_accuracy: 0.4074\n",
      "Epoch 11/150\n",
      "124/124 [==============================] - 0s 118us/step - loss: 0.9827 - accuracy: 0.5645 - val_loss: 1.0046 - val_accuracy: 0.4074\n",
      "Epoch 12/150\n",
      "124/124 [==============================] - 0s 99us/step - loss: 0.9717 - accuracy: 0.5726 - val_loss: 0.9957 - val_accuracy: 0.4074\n",
      "Epoch 13/150\n",
      "124/124 [==============================] - 0s 111us/step - loss: 0.9602 - accuracy: 0.5887 - val_loss: 0.9870 - val_accuracy: 0.4259\n",
      "Epoch 14/150\n",
      "124/124 [==============================] - 0s 81us/step - loss: 0.9498 - accuracy: 0.5887 - val_loss: 0.9782 - val_accuracy: 0.4444\n",
      "Epoch 15/150\n",
      "124/124 [==============================] - 0s 94us/step - loss: 0.9389 - accuracy: 0.5887 - val_loss: 0.9692 - val_accuracy: 0.4630\n",
      "Epoch 16/150\n",
      "124/124 [==============================] - 0s 116us/step - loss: 0.9284 - accuracy: 0.5887 - val_loss: 0.9601 - val_accuracy: 0.4815\n",
      "Epoch 17/150\n",
      "124/124 [==============================] - 0s 107us/step - loss: 0.9175 - accuracy: 0.5968 - val_loss: 0.9511 - val_accuracy: 0.5000\n",
      "Epoch 18/150\n",
      "124/124 [==============================] - 0s 121us/step - loss: 0.9071 - accuracy: 0.6048 - val_loss: 0.9421 - val_accuracy: 0.5000\n",
      "Epoch 19/150\n",
      "124/124 [==============================] - 0s 99us/step - loss: 0.8970 - accuracy: 0.6048 - val_loss: 0.9333 - val_accuracy: 0.4815\n",
      "Epoch 20/150\n",
      "124/124 [==============================] - 0s 103us/step - loss: 0.8867 - accuracy: 0.6048 - val_loss: 0.9244 - val_accuracy: 0.5000\n",
      "Epoch 21/150\n",
      "124/124 [==============================] - 0s 99us/step - loss: 0.8765 - accuracy: 0.6129 - val_loss: 0.9157 - val_accuracy: 0.5000\n",
      "Epoch 22/150\n",
      "124/124 [==============================] - 0s 87us/step - loss: 0.8663 - accuracy: 0.6129 - val_loss: 0.9070 - val_accuracy: 0.5185\n",
      "Epoch 23/150\n",
      "124/124 [==============================] - 0s 192us/step - loss: 0.8569 - accuracy: 0.6129 - val_loss: 0.8981 - val_accuracy: 0.5185\n",
      "Epoch 24/150\n",
      "124/124 [==============================] - 0s 133us/step - loss: 0.8472 - accuracy: 0.6129 - val_loss: 0.8892 - val_accuracy: 0.5185\n",
      "Epoch 25/150\n",
      "124/124 [==============================] - 0s 93us/step - loss: 0.8380 - accuracy: 0.6210 - val_loss: 0.8804 - val_accuracy: 0.5185\n",
      "Epoch 26/150\n",
      "124/124 [==============================] - 0s 101us/step - loss: 0.8283 - accuracy: 0.6210 - val_loss: 0.8714 - val_accuracy: 0.5556\n",
      "Epoch 27/150\n",
      "124/124 [==============================] - 0s 138us/step - loss: 0.8188 - accuracy: 0.6290 - val_loss: 0.8625 - val_accuracy: 0.5741\n",
      "Epoch 28/150\n",
      "124/124 [==============================] - 0s 122us/step - loss: 0.8093 - accuracy: 0.6290 - val_loss: 0.8535 - val_accuracy: 0.5926\n",
      "Epoch 29/150\n",
      "124/124 [==============================] - 0s 117us/step - loss: 0.8000 - accuracy: 0.6452 - val_loss: 0.8443 - val_accuracy: 0.6111\n",
      "Epoch 30/150\n",
      "124/124 [==============================] - 0s 127us/step - loss: 0.7905 - accuracy: 0.6532 - val_loss: 0.8354 - val_accuracy: 0.6296\n",
      "Epoch 31/150\n",
      "124/124 [==============================] - 0s 115us/step - loss: 0.7810 - accuracy: 0.6613 - val_loss: 0.8265 - val_accuracy: 0.6481\n",
      "Epoch 32/150\n",
      "124/124 [==============================] - 0s 116us/step - loss: 0.7720 - accuracy: 0.6613 - val_loss: 0.8178 - val_accuracy: 0.6481\n",
      "Epoch 33/150\n",
      "124/124 [==============================] - 0s 118us/step - loss: 0.7631 - accuracy: 0.6694 - val_loss: 0.8096 - val_accuracy: 0.6667\n",
      "Epoch 34/150\n",
      "124/124 [==============================] - 0s 109us/step - loss: 0.7543 - accuracy: 0.6774 - val_loss: 0.8015 - val_accuracy: 0.6667\n",
      "Epoch 35/150\n",
      "124/124 [==============================] - 0s 122us/step - loss: 0.7456 - accuracy: 0.6774 - val_loss: 0.7929 - val_accuracy: 0.6667\n",
      "Epoch 36/150\n",
      "124/124 [==============================] - 0s 75us/step - loss: 0.7371 - accuracy: 0.7016 - val_loss: 0.7842 - val_accuracy: 0.6667\n",
      "Epoch 37/150\n",
      "124/124 [==============================] - 0s 106us/step - loss: 0.7288 - accuracy: 0.7097 - val_loss: 0.7759 - val_accuracy: 0.6852\n",
      "Epoch 38/150\n",
      "124/124 [==============================] - 0s 86us/step - loss: 0.7210 - accuracy: 0.7258 - val_loss: 0.7681 - val_accuracy: 0.7037\n",
      "Epoch 39/150\n",
      "124/124 [==============================] - 0s 81us/step - loss: 0.7130 - accuracy: 0.7419 - val_loss: 0.7608 - val_accuracy: 0.7222\n",
      "Epoch 40/150\n",
      "124/124 [==============================] - 0s 66us/step - loss: 0.7053 - accuracy: 0.7419 - val_loss: 0.7536 - val_accuracy: 0.7407\n",
      "Epoch 41/150\n",
      "124/124 [==============================] - 0s 63us/step - loss: 0.6978 - accuracy: 0.7500 - val_loss: 0.7461 - val_accuracy: 0.7407\n",
      "Epoch 42/150\n",
      "124/124 [==============================] - 0s 90us/step - loss: 0.6901 - accuracy: 0.7500 - val_loss: 0.7381 - val_accuracy: 0.7407\n",
      "Epoch 43/150\n",
      "124/124 [==============================] - 0s 92us/step - loss: 0.6821 - accuracy: 0.7500 - val_loss: 0.7300 - val_accuracy: 0.7407\n",
      "Epoch 44/150\n",
      "124/124 [==============================] - 0s 57us/step - loss: 0.6740 - accuracy: 0.7500 - val_loss: 0.7222 - val_accuracy: 0.7593\n",
      "Epoch 45/150\n",
      "124/124 [==============================] - 0s 71us/step - loss: 0.6662 - accuracy: 0.7581 - val_loss: 0.7142 - val_accuracy: 0.7593\n",
      "Epoch 46/150\n",
      "124/124 [==============================] - 0s 92us/step - loss: 0.6579 - accuracy: 0.7581 - val_loss: 0.7062 - val_accuracy: 0.7593\n",
      "Epoch 47/150\n",
      "124/124 [==============================] - 0s 73us/step - loss: 0.6498 - accuracy: 0.7581 - val_loss: 0.6982 - val_accuracy: 0.7593\n",
      "Epoch 48/150\n",
      "124/124 [==============================] - 0s 62us/step - loss: 0.6415 - accuracy: 0.7742 - val_loss: 0.6903 - val_accuracy: 0.7593\n",
      "Epoch 49/150\n",
      "124/124 [==============================] - 0s 67us/step - loss: 0.6333 - accuracy: 0.7742 - val_loss: 0.6822 - val_accuracy: 0.7593\n",
      "Epoch 50/150\n",
      "124/124 [==============================] - 0s 79us/step - loss: 0.6252 - accuracy: 0.7823 - val_loss: 0.6740 - val_accuracy: 0.7778\n",
      "Epoch 51/150\n",
      "124/124 [==============================] - 0s 62us/step - loss: 0.6168 - accuracy: 0.7903 - val_loss: 0.6659 - val_accuracy: 0.7963\n",
      "Epoch 52/150\n",
      "124/124 [==============================] - 0s 71us/step - loss: 0.6088 - accuracy: 0.7984 - val_loss: 0.6577 - val_accuracy: 0.7963\n",
      "Epoch 53/150\n",
      "124/124 [==============================] - 0s 94us/step - loss: 0.6010 - accuracy: 0.8145 - val_loss: 0.6491 - val_accuracy: 0.8148\n",
      "Epoch 54/150\n",
      "124/124 [==============================] - 0s 77us/step - loss: 0.5931 - accuracy: 0.8226 - val_loss: 0.6399 - val_accuracy: 0.8333\n",
      "Epoch 55/150\n",
      "124/124 [==============================] - 0s 79us/step - loss: 0.5850 - accuracy: 0.8226 - val_loss: 0.6308 - val_accuracy: 0.8519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/150\n",
      "124/124 [==============================] - 0s 67us/step - loss: 0.5771 - accuracy: 0.8306 - val_loss: 0.6219 - val_accuracy: 0.8519\n",
      "Epoch 57/150\n",
      "124/124 [==============================] - 0s 91us/step - loss: 0.5691 - accuracy: 0.8306 - val_loss: 0.6131 - val_accuracy: 0.8519\n",
      "Epoch 58/150\n",
      "124/124 [==============================] - 0s 96us/step - loss: 0.5608 - accuracy: 0.8387 - val_loss: 0.6042 - val_accuracy: 0.8519\n",
      "Epoch 59/150\n",
      "124/124 [==============================] - 0s 73us/step - loss: 0.5528 - accuracy: 0.8387 - val_loss: 0.5957 - val_accuracy: 0.8519\n",
      "Epoch 60/150\n",
      "124/124 [==============================] - 0s 58us/step - loss: 0.5446 - accuracy: 0.8468 - val_loss: 0.5872 - val_accuracy: 0.8704\n",
      "Epoch 61/150\n",
      "124/124 [==============================] - 0s 106us/step - loss: 0.5362 - accuracy: 0.8468 - val_loss: 0.5787 - val_accuracy: 0.8704\n",
      "Epoch 62/150\n",
      "124/124 [==============================] - 0s 88us/step - loss: 0.5285 - accuracy: 0.8468 - val_loss: 0.5700 - val_accuracy: 0.8704\n",
      "Epoch 63/150\n",
      "124/124 [==============================] - 0s 88us/step - loss: 0.5206 - accuracy: 0.8468 - val_loss: 0.5610 - val_accuracy: 0.8704\n",
      "Epoch 64/150\n",
      "124/124 [==============================] - 0s 73us/step - loss: 0.5128 - accuracy: 0.8468 - val_loss: 0.5520 - val_accuracy: 0.8704\n",
      "Epoch 65/150\n",
      "124/124 [==============================] - 0s 71us/step - loss: 0.5048 - accuracy: 0.8468 - val_loss: 0.5433 - val_accuracy: 0.8704\n",
      "Epoch 66/150\n",
      "124/124 [==============================] - 0s 86us/step - loss: 0.4972 - accuracy: 0.8468 - val_loss: 0.5344 - val_accuracy: 0.8889\n",
      "Epoch 67/150\n",
      "124/124 [==============================] - 0s 93us/step - loss: 0.4899 - accuracy: 0.8468 - val_loss: 0.5257 - val_accuracy: 0.8889\n",
      "Epoch 68/150\n",
      "124/124 [==============================] - 0s 71us/step - loss: 0.4825 - accuracy: 0.8548 - val_loss: 0.5173 - val_accuracy: 0.8889\n",
      "Epoch 69/150\n",
      "124/124 [==============================] - 0s 71us/step - loss: 0.4751 - accuracy: 0.8548 - val_loss: 0.5092 - val_accuracy: 0.8889\n",
      "Epoch 70/150\n",
      "124/124 [==============================] - 0s 85us/step - loss: 0.4677 - accuracy: 0.8710 - val_loss: 0.5015 - val_accuracy: 0.8889\n",
      "Epoch 71/150\n",
      "124/124 [==============================] - 0s 93us/step - loss: 0.4606 - accuracy: 0.8710 - val_loss: 0.4940 - val_accuracy: 0.8889\n",
      "Epoch 72/150\n",
      "124/124 [==============================] - 0s 65us/step - loss: 0.4538 - accuracy: 0.8710 - val_loss: 0.4867 - val_accuracy: 0.8889\n",
      "Epoch 73/150\n",
      "124/124 [==============================] - 0s 71us/step - loss: 0.4470 - accuracy: 0.8710 - val_loss: 0.4796 - val_accuracy: 0.9074\n",
      "Epoch 74/150\n",
      "124/124 [==============================] - 0s 75us/step - loss: 0.4404 - accuracy: 0.8710 - val_loss: 0.4723 - val_accuracy: 0.9259\n",
      "Epoch 75/150\n",
      "124/124 [==============================] - 0s 93us/step - loss: 0.4337 - accuracy: 0.8710 - val_loss: 0.4652 - val_accuracy: 0.9259\n",
      "Epoch 76/150\n",
      "124/124 [==============================] - 0s 77us/step - loss: 0.4270 - accuracy: 0.8790 - val_loss: 0.4581 - val_accuracy: 0.9259\n",
      "Epoch 77/150\n",
      "124/124 [==============================] - 0s 67us/step - loss: 0.4207 - accuracy: 0.8871 - val_loss: 0.4505 - val_accuracy: 0.9259\n",
      "Epoch 78/150\n",
      "124/124 [==============================] - 0s 79us/step - loss: 0.4140 - accuracy: 0.8871 - val_loss: 0.4423 - val_accuracy: 0.9630\n",
      "Epoch 79/150\n",
      "124/124 [==============================] - 0s 73us/step - loss: 0.4074 - accuracy: 0.8952 - val_loss: 0.4341 - val_accuracy: 0.9630\n",
      "Epoch 80/150\n",
      "124/124 [==============================] - 0s 83us/step - loss: 0.4006 - accuracy: 0.8952 - val_loss: 0.4262 - val_accuracy: 0.9630\n",
      "Epoch 81/150\n",
      "124/124 [==============================] - 0s 61us/step - loss: 0.3939 - accuracy: 0.8952 - val_loss: 0.4184 - val_accuracy: 0.9630\n",
      "Epoch 82/150\n",
      "124/124 [==============================] - 0s 63us/step - loss: 0.3874 - accuracy: 0.9032 - val_loss: 0.4103 - val_accuracy: 0.9630\n",
      "Epoch 83/150\n",
      "124/124 [==============================] - 0s 84us/step - loss: 0.3805 - accuracy: 0.9032 - val_loss: 0.4018 - val_accuracy: 0.9630\n",
      "Epoch 84/150\n",
      "124/124 [==============================] - 0s 88us/step - loss: 0.3738 - accuracy: 0.9032 - val_loss: 0.3933 - val_accuracy: 0.9630\n",
      "Epoch 85/150\n",
      "124/124 [==============================] - 0s 71us/step - loss: 0.3666 - accuracy: 0.9113 - val_loss: 0.3849 - val_accuracy: 0.9630\n",
      "Epoch 86/150\n",
      "124/124 [==============================] - 0s 67us/step - loss: 0.3597 - accuracy: 0.9113 - val_loss: 0.3770 - val_accuracy: 0.9630\n",
      "Epoch 87/150\n",
      "124/124 [==============================] - 0s 92us/step - loss: 0.3532 - accuracy: 0.9194 - val_loss: 0.3694 - val_accuracy: 0.9815\n",
      "Epoch 88/150\n",
      "124/124 [==============================] - 0s 75us/step - loss: 0.3469 - accuracy: 0.9194 - val_loss: 0.3616 - val_accuracy: 0.9815\n",
      "Epoch 89/150\n",
      "124/124 [==============================] - 0s 60us/step - loss: 0.3405 - accuracy: 0.9113 - val_loss: 0.3539 - val_accuracy: 0.9815\n",
      "Epoch 90/150\n",
      "124/124 [==============================] - 0s 73us/step - loss: 0.3341 - accuracy: 0.9194 - val_loss: 0.3465 - val_accuracy: 0.9815\n",
      "Epoch 91/150\n",
      "124/124 [==============================] - 0s 77us/step - loss: 0.3281 - accuracy: 0.9194 - val_loss: 0.3392 - val_accuracy: 0.9815\n",
      "Epoch 92/150\n",
      "124/124 [==============================] - 0s 75us/step - loss: 0.3217 - accuracy: 0.9194 - val_loss: 0.3323 - val_accuracy: 0.9815\n",
      "Epoch 93/150\n",
      "124/124 [==============================] - 0s 53us/step - loss: 0.3159 - accuracy: 0.9194 - val_loss: 0.3255 - val_accuracy: 0.9815\n",
      "Epoch 94/150\n",
      "124/124 [==============================] - 0s 71us/step - loss: 0.3103 - accuracy: 0.9194 - val_loss: 0.3187 - val_accuracy: 0.9815\n",
      "Epoch 95/150\n",
      "124/124 [==============================] - 0s 89us/step - loss: 0.3046 - accuracy: 0.9194 - val_loss: 0.3118 - val_accuracy: 0.9815\n",
      "Epoch 96/150\n",
      "124/124 [==============================] - 0s 80us/step - loss: 0.2987 - accuracy: 0.9274 - val_loss: 0.3052 - val_accuracy: 0.9815\n",
      "Epoch 97/150\n",
      "124/124 [==============================] - 0s 101us/step - loss: 0.2931 - accuracy: 0.9274 - val_loss: 0.2988 - val_accuracy: 0.9815\n",
      "Epoch 98/150\n",
      "124/124 [==============================] - 0s 75us/step - loss: 0.2876 - accuracy: 0.9274 - val_loss: 0.2928 - val_accuracy: 0.9815\n",
      "Epoch 99/150\n",
      "124/124 [==============================] - 0s 66us/step - loss: 0.2824 - accuracy: 0.9274 - val_loss: 0.2871 - val_accuracy: 0.9815\n",
      "Epoch 100/150\n",
      "124/124 [==============================] - 0s 91us/step - loss: 0.2771 - accuracy: 0.9274 - val_loss: 0.2814 - val_accuracy: 0.9815\n",
      "Epoch 101/150\n",
      "124/124 [==============================] - 0s 68us/step - loss: 0.2720 - accuracy: 0.9274 - val_loss: 0.2757 - val_accuracy: 0.9815\n",
      "Epoch 102/150\n",
      "124/124 [==============================] - 0s 70us/step - loss: 0.2669 - accuracy: 0.9274 - val_loss: 0.2702 - val_accuracy: 0.9815\n",
      "Epoch 103/150\n",
      "124/124 [==============================] - 0s 79us/step - loss: 0.2619 - accuracy: 0.9274 - val_loss: 0.2652 - val_accuracy: 0.9815\n",
      "Epoch 104/150\n",
      "124/124 [==============================] - 0s 76us/step - loss: 0.2572 - accuracy: 0.9274 - val_loss: 0.2604 - val_accuracy: 0.9815\n",
      "Epoch 105/150\n",
      "124/124 [==============================] - 0s 51us/step - loss: 0.2527 - accuracy: 0.9274 - val_loss: 0.2557 - val_accuracy: 0.9815\n",
      "Epoch 106/150\n",
      "124/124 [==============================] - 0s 70us/step - loss: 0.2484 - accuracy: 0.9274 - val_loss: 0.2509 - val_accuracy: 0.9815\n",
      "Epoch 107/150\n",
      "124/124 [==============================] - 0s 98us/step - loss: 0.2439 - accuracy: 0.9274 - val_loss: 0.2463 - val_accuracy: 0.9815\n",
      "Epoch 108/150\n",
      "124/124 [==============================] - 0s 86us/step - loss: 0.2395 - accuracy: 0.9274 - val_loss: 0.2419 - val_accuracy: 0.9815\n",
      "Epoch 109/150\n",
      "124/124 [==============================] - 0s 79us/step - loss: 0.2351 - accuracy: 0.9274 - val_loss: 0.2372 - val_accuracy: 0.9815\n",
      "Epoch 110/150\n",
      "124/124 [==============================] - 0s 73us/step - loss: 0.2306 - accuracy: 0.9355 - val_loss: 0.2323 - val_accuracy: 0.9815\n",
      "Epoch 111/150\n",
      "124/124 [==============================] - 0s 65us/step - loss: 0.2260 - accuracy: 0.9355 - val_loss: 0.2275 - val_accuracy: 0.9815\n",
      "Epoch 112/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 0s 128us/step - loss: 0.2215 - accuracy: 0.9355 - val_loss: 0.2227 - val_accuracy: 0.9815\n",
      "Epoch 113/150\n",
      "124/124 [==============================] - 0s 96us/step - loss: 0.2173 - accuracy: 0.9355 - val_loss: 0.2181 - val_accuracy: 0.9815\n",
      "Epoch 114/150\n",
      "124/124 [==============================] - 0s 75us/step - loss: 0.2130 - accuracy: 0.9355 - val_loss: 0.2139 - val_accuracy: 0.9815\n",
      "Epoch 115/150\n",
      "124/124 [==============================] - 0s 83us/step - loss: 0.2091 - accuracy: 0.9355 - val_loss: 0.2100 - val_accuracy: 0.9815\n",
      "Epoch 116/150\n",
      "124/124 [==============================] - 0s 69us/step - loss: 0.2052 - accuracy: 0.9355 - val_loss: 0.2060 - val_accuracy: 0.9815\n",
      "Epoch 117/150\n",
      "124/124 [==============================] - 0s 73us/step - loss: 0.2015 - accuracy: 0.9435 - val_loss: 0.2021 - val_accuracy: 0.9815\n",
      "Epoch 118/150\n",
      "124/124 [==============================] - 0s 109us/step - loss: 0.1976 - accuracy: 0.9516 - val_loss: 0.1984 - val_accuracy: 0.9815\n",
      "Epoch 119/150\n",
      "124/124 [==============================] - 0s 91us/step - loss: 0.1941 - accuracy: 0.9597 - val_loss: 0.1948 - val_accuracy: 0.9815\n",
      "Epoch 120/150\n",
      "124/124 [==============================] - 0s 70us/step - loss: 0.1905 - accuracy: 0.9597 - val_loss: 0.1915 - val_accuracy: 0.9815\n",
      "Epoch 121/150\n",
      "124/124 [==============================] - 0s 63us/step - loss: 0.1871 - accuracy: 0.9677 - val_loss: 0.1882 - val_accuracy: 0.9815\n",
      "Epoch 122/150\n",
      "124/124 [==============================] - 0s 86us/step - loss: 0.1836 - accuracy: 0.9677 - val_loss: 0.1851 - val_accuracy: 0.9815\n",
      "Epoch 123/150\n",
      "124/124 [==============================] - 0s 75us/step - loss: 0.1803 - accuracy: 0.9677 - val_loss: 0.1822 - val_accuracy: 0.9815\n",
      "Epoch 124/150\n",
      "124/124 [==============================] - 0s 56us/step - loss: 0.1770 - accuracy: 0.9677 - val_loss: 0.1794 - val_accuracy: 0.9815\n",
      "Epoch 125/150\n",
      "124/124 [==============================] - 0s 79us/step - loss: 0.1738 - accuracy: 0.9677 - val_loss: 0.1766 - val_accuracy: 0.9815\n",
      "Epoch 126/150\n",
      "124/124 [==============================] - 0s 87us/step - loss: 0.1704 - accuracy: 0.9758 - val_loss: 0.1737 - val_accuracy: 0.9815\n",
      "Epoch 127/150\n",
      "124/124 [==============================] - 0s 66us/step - loss: 0.1673 - accuracy: 0.9758 - val_loss: 0.1708 - val_accuracy: 0.9815\n",
      "Epoch 128/150\n",
      "124/124 [==============================] - 0s 65us/step - loss: 0.1643 - accuracy: 0.9758 - val_loss: 0.1681 - val_accuracy: 0.9815\n",
      "Epoch 129/150\n",
      "124/124 [==============================] - 0s 76us/step - loss: 0.1612 - accuracy: 0.9758 - val_loss: 0.1650 - val_accuracy: 0.9815\n",
      "Epoch 130/150\n",
      "124/124 [==============================] - 0s 87us/step - loss: 0.1580 - accuracy: 0.9758 - val_loss: 0.1622 - val_accuracy: 0.9815\n",
      "Epoch 131/150\n",
      "124/124 [==============================] - 0s 72us/step - loss: 0.1549 - accuracy: 0.9758 - val_loss: 0.1596 - val_accuracy: 0.9815\n",
      "Epoch 132/150\n",
      "124/124 [==============================] - 0s 70us/step - loss: 0.1520 - accuracy: 0.9758 - val_loss: 0.1571 - val_accuracy: 0.9815\n",
      "Epoch 133/150\n",
      "124/124 [==============================] - 0s 99us/step - loss: 0.1492 - accuracy: 0.9758 - val_loss: 0.1546 - val_accuracy: 0.9815\n",
      "Epoch 134/150\n",
      "124/124 [==============================] - 0s 107us/step - loss: 0.1463 - accuracy: 0.9758 - val_loss: 0.1524 - val_accuracy: 0.9815\n",
      "Epoch 135/150\n",
      "124/124 [==============================] - 0s 85us/step - loss: 0.1435 - accuracy: 0.9758 - val_loss: 0.1501 - val_accuracy: 0.9815\n",
      "Epoch 136/150\n",
      "124/124 [==============================] - 0s 74us/step - loss: 0.1408 - accuracy: 0.9758 - val_loss: 0.1480 - val_accuracy: 0.9815\n",
      "Epoch 137/150\n",
      "124/124 [==============================] - 0s 74us/step - loss: 0.1382 - accuracy: 0.9839 - val_loss: 0.1459 - val_accuracy: 0.9815\n",
      "Epoch 138/150\n",
      "124/124 [==============================] - 0s 89us/step - loss: 0.1360 - accuracy: 0.9839 - val_loss: 0.1437 - val_accuracy: 0.9815\n",
      "Epoch 139/150\n",
      "124/124 [==============================] - 0s 78us/step - loss: 0.1336 - accuracy: 0.9839 - val_loss: 0.1416 - val_accuracy: 0.9815\n",
      "Epoch 140/150\n",
      "124/124 [==============================] - 0s 65us/step - loss: 0.1314 - accuracy: 0.9839 - val_loss: 0.1396 - val_accuracy: 0.9815\n",
      "Epoch 141/150\n",
      "124/124 [==============================] - 0s 85us/step - loss: 0.1290 - accuracy: 0.9839 - val_loss: 0.1380 - val_accuracy: 0.9815\n",
      "Epoch 142/150\n",
      "124/124 [==============================] - 0s 85us/step - loss: 0.1268 - accuracy: 0.9839 - val_loss: 0.1364 - val_accuracy: 0.9815\n",
      "Epoch 143/150\n",
      "124/124 [==============================] - 0s 72us/step - loss: 0.1247 - accuracy: 0.9839 - val_loss: 0.1348 - val_accuracy: 0.9815\n",
      "Epoch 144/150\n",
      "124/124 [==============================] - 0s 65us/step - loss: 0.1224 - accuracy: 0.9839 - val_loss: 0.1334 - val_accuracy: 0.9815\n",
      "Epoch 145/150\n",
      "124/124 [==============================] - 0s 70us/step - loss: 0.1202 - accuracy: 0.9839 - val_loss: 0.1321 - val_accuracy: 0.9815\n",
      "Epoch 146/150\n",
      "124/124 [==============================] - 0s 78us/step - loss: 0.1180 - accuracy: 0.9839 - val_loss: 0.1308 - val_accuracy: 0.9815\n",
      "Epoch 147/150\n",
      "124/124 [==============================] - 0s 116us/step - loss: 0.1160 - accuracy: 0.9839 - val_loss: 0.1294 - val_accuracy: 0.9815\n",
      "Epoch 148/150\n",
      "124/124 [==============================] - 0s 89us/step - loss: 0.1139 - accuracy: 0.9839 - val_loss: 0.1279 - val_accuracy: 0.9815\n",
      "Epoch 149/150\n",
      "124/124 [==============================] - 0s 88us/step - loss: 0.1121 - accuracy: 0.9839 - val_loss: 0.1265 - val_accuracy: 0.9815\n",
      "Epoch 150/150\n",
      "124/124 [==============================] - 0s 60us/step - loss: 0.1104 - accuracy: 0.9839 - val_loss: 0.1251 - val_accuracy: 0.9815\n",
      "Test loss: 0.12507736075807502\n",
      "Test accuracy: 0.9814814925193787\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=150,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
